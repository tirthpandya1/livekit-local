# Docker Compose Project Name
COMPOSE_PROJECT_NAME=livekit_voice_ai_stack

# LiveKit Server Configuration
LIVEKIT_API_KEY=devkey
LIVEKIT_API_SECRET=secret
LIVEKIT_SERVER_URL_INTERNAL=ws://livekit:7880
LIVEKIT_SERVER_URL_EXTERNAL=ws://localhost:7880 # For frontend access from host browser
LIVEKIT_TCP_PORT=7881
LIVEKIT_UDP_PORT_START=5000
LIVEKIT_UDP_PORT_END=6000

# Orpheus TTS API (orpheus-fastapi service)
# ORPHEUS_TTS_API_INTERNAL_URL is used by the agent
ORPHEUS_TTS_API_INTERNAL_URL=http://orpheus-tts-api:5005/v1

# Orpheus TTS Model Backend (orpheus-tts-model service, llama.cpp based)
# This URL is used by the orpheus-tts-api service
ORPHEUS_TTS_MODEL_BACKEND_INTERNAL_URL=http://orpheus-tts-model:5006/v1/completions
ORPHEUS_MODEL_NAME=Orpheus-3b-FT-Q8_0.gguf # Or your preferred Orpheus model
ORPHEUS_MAX_TOKENS=2048
# For model downloader (ensure these match your host user if issues arise, especially on Linux)
# UID=1000
# GID=1000

# Faster Whisper STT Server (stt-api service)
# STT_API_INTERNAL_URL is used by the agent
STT_API_INTERNAL_URL=http://stt-api:8000/v1
WHISPER_MODEL_NAME="Systran/faster-whisper-large-v3"
WHISPER_INFERENCE_DEVICE="cuda" # "cuda" or "cpu"
WHISPER_COMPUTE_TYPE="float16" # e.g., "float16", "int8"

# Ollama LLM Server (ollama service)
# LLM_API_INTERNAL_URL is used by the agent
LLM_API_INTERNAL_URL=http://ollama:11434/v1
OLLAMA_LLM_MODEL_TO_PULL="llama3.2:3b" # Agent will request this model from Ollama

# Local Voice AI Agent (local-agent service)
# Agent specific settings if any, beyond the service URLs which are set via environment in docker-compose

# Frontend (frontend service - local-voice-ai/voice-assistant-frontend)
# NEXT_PUBLIC_LIVEKIT_URL is used by the client-side JS in the browser
# It must point to where the LiveKit server is accessible from the user's machine (localhost)
NEXT_PUBLIC_LIVEKIT_URL=${LIVEKIT_SERVER_URL_EXTERNAL}
# The frontend's server-side API for connection details will use LIVEKIT_API_KEY and LIVEKIT_API_SECRET.